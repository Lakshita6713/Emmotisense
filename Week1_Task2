import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
# Load IRIS dataset
iris = load_iris()
X = iris['data']
y = iris['target']
# Keep only versicolor (1) and virginica (2)
mask = (y == 1) | (y == 2)
X = X[mask]
y = y[mask]
y = y - 1  # Convert to 0 and 1
# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add bias column
X = np.c_[np.ones((X.shape[0], 1)), X]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Cost function (optional)
def cost_function(X, y, beta):
    m = len(y)
    predictions = sigmoid(X @ beta)
    cost = - (1/m) * np.sum(y * np.log(predictions + 1e-10) + (1 - y) * np.log(1 - predictions + 1e-10))
    return cost
# Gradient Descent
def logistic_regression(X, y, lr=0.1, epochs=1000):
    beta = np.zeros(X.shape[1])
    m = len(y)

    for i in range(epochs):
        predictions = sigmoid(X @ beta)
        gradient = (1 / m) * (X.T @ (predictions - y))
        beta -= lr * gradient
    return beta

# Train model
beta = logistic_regression(X_train, y_train)

# Predict
preds = sigmoid(X_test @ beta) >= 0.5
accuracy = accuracy_score(y_test, preds)

print(f"Accuracy on versicolor vs virginica: {accuracy:.3f}")
